# fusion.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class BilinearAttentionPooling(nn.Module):
    def __init__(self, feature1_dim, feature2_dim, hidden_dim):
        super(BilinearAttentionPooling, self).__init__()
        self.feature1_proj = nn.Linear(feature1_dim, hidden_dim)
        self.feature2_proj = nn.Linear(feature2_dim, hidden_dim)
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, feature1, feature2):
        # 投影特征到共同的隐藏空间
        proj_feature1 = self.feature1_proj(feature1)  # [batch_size, hidden_dim]
        proj_feature2 = self.feature2_proj(feature2)  # [batch_size, hidden_dim]

        # 双线性汇聚
        bilinear = proj_feature1 * proj_feature2  # [batch_size, hidden_dim]

        # 计算注意力权重
        attention_weights = F.softmax(self.attention(bilinear), dim=1)  # [batch_size, 1]

        # 加权汇聚
        fused_features = attention_weights * bilinear  # [batch_size, hidden_dim]

        return fused_features

# 初始化 BAN 融合模块


# 训练循环中使用 BAN 进行融合


