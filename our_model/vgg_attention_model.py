# vgg_attention_model.py
# import torch
# import torch.nn as nn
# from torchvision.models import vgg16

# class SelfAttention(nn.Module):
#     def __init__(self, in_channels, input_channels=None):
#         super(SelfAttention, self).__init__()
#         if input_channels is None:
#             input_channels = in_channels
#         self.query_conv = nn.Conv2d(input_channels, in_channels // 8, kernel_size=1)
#         self.key_conv = nn.Conv2d(input_channels, in_channels // 8, kernel_size=1)
#         self.value_conv = nn.Conv2d(input_channels, in_channels, kernel_size=1)
#         self.softmax = nn.Softmax(dim=-1)


#     def forward(self, x):
#         batch_size, C, width, height = x.size()
#         query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)
#         key = self.key_conv(x).view(batch_size, -1, width * height)
#         energy = torch.bmm(query, key)  # 计算能量值
#         attention = self.softmax(energy)  # 应用softmax得到注意力权重
#         value = self.value_conv(x).view(batch_size, -1, width * height)

#         out = torch.bmm(value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, width, height)

#         return out + x  # 返回加上原始输入的结果，实现残差连接

# class VGGAttentionModel(nn.Module):
#     def __init__(self, pretrained=True):
#         super(VGGAttentionModel, self).__init__()
#         original_vgg = vgg16(pretrained=pretrained)
#         self.features_conv = nn.Sequential(*list(original_vgg.features.children())[:17])  # 保留直到第三个最大池化层的特征
#         self.features_after_pool = nn.Sequential(*list(original_vgg.features.children())[17:])  # 第三个最大池化层之后的特征
#         self.attention = SelfAttention(256, input_channels=256)  # 添加自注意力模块

#     def forward(self, imgs):
#             # DataParallel might add an extra dimension for the replicas, so we handle both cases (with and without DataParallel)
#             if imgs.dim() == 5:
#                 batch_size, num_imgs, C, H, W = imgs.size()
#             elif imgs.dim() == 6:  # Considering an additional dimension for DataParallel
#                 replica_size, batch_size, num_imgs, C, H, W = imgs.size()
#                 imgs = imgs.view(-1, num_imgs, C, H, W)  # Flatten the first two dimensions
#             else:
#                 raise ValueError("Unexpected input shape for VGGAttentionModel")

#             concatenated_features = []
#             for i in range(num_imgs):
#                 x = imgs[:, i, :, :, :]
#                 x = self.features_conv(x)  # Using VGG to extract features up to the third max-pooling layer
#                 x = self.attention(x)  # Apply self-attention
#                 x = self.features_after_pool(x)  # Apply the remaining VGG feature layers
#                 x = x.view(x.size(0), -1)  # Flatten the features
#                 concatenated_features.append(x)

#             # Concatenate all image features
#             feature1 = torch.cat(concatenated_features, dim=1)

#             return feature1


import torch
import torch.nn as nn
from torchvision.models import vgg16

class SelfAttention(nn.Module):
    def __init__(self, in_channels, input_channels=None, dropout_rate=0.5):
        super(SelfAttention, self).__init__()
        if input_channels is None:
            input_channels = in_channels
        self.query_conv = nn.Conv2d(input_channels, in_channels // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(input_channels, in_channels // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(input_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.dropout = nn.Dropout(dropout_rate)  # 在自注意力模块中添加 Dropout

    def forward(self, x):
        batch_size, C, width, height = x.size()
        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)
        key = self.key_conv(x).view(batch_size, -1, width * height)
        energy = torch.bmm(query, key)  # 计算能量值
        attention = self.softmax(energy)  # 应用 softmax 得到注意力权重
        attention = self.dropout(attention)  # 应用 Dropout
        value = self.value_conv(x).view(batch_size, -1, width * height)

        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, width, height)

        return out + x  # 返回加上原始输入的结果，实现残差连接

class VGGAttentionModel(nn.Module):
    def __init__(self, pretrained=True, dropout_rate=0.5):
        super(VGGAttentionModel, self).__init__()
        original_vgg = vgg16(pretrained=pretrained)
        self.features_conv = nn.Sequential(*list(original_vgg.features.children())[:17])  # 保留到第三个最大池化层的特征
        self.attention = SelfAttention(256, input_channels=256, dropout_rate=dropout_rate)  # 自注意力模块
        self.dropout = nn.Dropout(dropout_rate)  # 在注意力模块和 features_after_pool 之间添加 Dropout
        self.features_after_pool = nn.Sequential(*list(original_vgg.features.children())[17:])  # 第三个最大池化层之后的特征

    def forward(self, imgs):
        # DataParallel might add an extra dimension for the replicas, so we handle both cases (with and without DataParallel)
        if imgs.dim() == 5:
            batch_size, num_imgs, C, H, W = imgs.size()
        elif imgs.dim() == 6:  # Considering an additional dimension for DataParallel
            replica_size, batch_size, num_imgs, C, H, W = imgs.size()
            imgs = imgs.view(-1, num_imgs, C, H, W)  # Flatten the first two dimensions
        else:
            raise ValueError("Unexpected input shape for VGGAttentionModel")

        concatenated_features = []
        for i in range(num_imgs):
            x = imgs[:, i, :, :, :]
            x = self.features_conv(x)  # 使用 VGG 提取特征
            x = self.attention(x)  # 应用自注意力
            x = self.dropout(x)  # 应用 Dropout
            x = self.features_after_pool(x)  # 应用剩余的 VGG 特征层
            x = x.view(x.size(0), -1)  # 展平特征
            concatenated_features.append(x)

        # Concatenate all image features
        feature1 = torch.cat(concatenated_features, dim=1)

        return feature1

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
# Example usage
# model = VGGAttentionModel(pretrained=True)
# model.to(device)  # Move to GPU if available
# output = model(input_tensor)  # input_tensor shape: [batch_size, 8, 3, 224, 224]

# 蒸馏模型》